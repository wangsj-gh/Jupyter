{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "import numba\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n",
      "Tue Aug 17 21:38:52 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 431.07       Driver Version: 431.07       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 106... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 38%   38C    P8     9W / 120W |   1779MiB /  6144MiB |     20%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1356    C+G   Insufficient Permissions                   N/A      |\n",
      "|    0      7860    C+G   C:\\Windows\\explorer.exe                    N/A      |\n",
      "|    0      8272    C+G   ...5n1h2txyewy\\StartMenuExperienceHost.exe N/A      |\n",
      "|    0      8676    C+G   ...dows.Search_cw5n1h2txyewy\\SearchApp.exe N/A      |\n",
      "|    0      9172    C+G   ...dows.Search_cw5n1h2txyewy\\SearchApp.exe N/A      |\n",
      "|    0      9228    C+G   ....150.0_x64__8wekyb3d8bbwe\\YourPhone.exe N/A      |\n",
      "|    0      9664    C+G   ...w5n1h2txyewy\\InputApp\\TextInputHost.exe N/A      |\n",
      "|    0     12396    C+G   ...ns\\XWeb\\338\\extracted\\WechatBrowser.exe N/A      |\n",
      "|    0     13684    C+G   ...5.0_x64__8wekyb3d8bbwe\\WinStore.App.exe N/A      |\n",
      "|    0     22028    C+G   ...t_cw5n1h2txyewy\\ShellExperienceHost.exe N/A      |\n",
      "|    0     28100    C+G   ...osoft.LockApp_cw5n1h2txyewy\\LockApp.exe N/A      |\n",
      "|    0     37836    C+G   ...21302.0_x64__atj5cpqqdhzyp\\QyClient.exe N/A      |\n",
      "|    0     67272    C+G   ...osoft Office\\root\\Office16\\POWERPNT.EXE N/A      |\n",
      "|    0    135584    C+G   ...al\\Google\\Chrome\\Application\\chrome.exe N/A      |\n",
      "|    0    163984    C+G   ...mmersiveControlPanel\\SystemSettings.exe N/A      |\n",
      "|    0    268804    C+G   ...lugins\\XWeb\\338\\extracted\\WeChatApp.exe N/A      |\n",
      "|    0    273352    C+G   ...)\\Microsoft\\Edge\\Application\\msedge.exe N/A      |\n",
      "|    0    286908      C   D:\\Miniconda3\\python.exe                   N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(cuda.gpus)\n",
    "# print(subprocess.check_output(\"nvidia-smi\",universal_newlines=True))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法1，最原始的算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time 8.356493949890137\n"
     ]
    }
   ],
   "source": [
    "# 最原始的矩阵相乘算法\n",
    "def matmul_cpu(A, B, C):\n",
    "    # 根据矩阵乘法原理，可以先根据A、B的尺寸算出结果矩阵C的大小\n",
    "    # 矩阵乘法结果C的大小是A的行数(shape[0])、B的列数(shape[1])\n",
    "    # 然后循环遍历C中的每个元素，分别获得A、B中对应行列的元素，相乘并累加即可\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            tmp = 0.0\n",
    "            # 根据矩阵乘法法则，A的列数(shape[1])应该和B的行数(shape[0])相等\n",
    "            # 所以这里A.shape[1]和B.shape[0]是等价的\n",
    "            for k in range(A.shape[1]):\n",
    "                # A、B中行列的对应元素相乘\n",
    "                tmp += A[i, k] * B[k, j]\n",
    "            # 最后将累加的结果放到对应位置中，完成一个元素的计算\n",
    "            C[i, j] = tmp\n",
    "if __name__ == '__main__':\n",
    "    # 待计算的矩阵\n",
    "    # 需要注意的还是shape的顺序问题，shape[0]表示行方向，shape[1]表示列方向\n",
    "    A = np.full([200, 300], 3, np.float32)\n",
    "    B = np.full([300, 200], 5, np.float32)\n",
    "    C_cpu_result = np.zeros([A.shape[0], B.shape[1]], np.float32)\n",
    "    start_time = time.time()\n",
    "    result=matmul_cpu(A, B, C_cpu_result)\n",
    "    end_time = time.time()\n",
    "    del result\n",
    "    print ('cpu time', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法2，利用Numba进行CPU加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time(Numba) 0.5038647651672363\n"
     ]
    }
   ],
   "source": [
    "# 调用Numba开启CPU并行加速\n",
    "# 其实和原始算法比，代码内容没有做任何改动，只是在函数名称前加了Numba的修饰符即可实现CPU加速\n",
    "@numba.jit(nopython=True)\n",
    "def matmul_cpu_numba(A, B, C):\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            tmp = 0.0\n",
    "            for k in range(A.shape[1]):\n",
    "                tmp += A[i, k] * B[k, j]\n",
    "            C[i, j] = tmp\n",
    "if __name__ == '__main__': \n",
    "    A = np.full([200, 300], 3, np.float32)\n",
    "    B = np.full([300, 200], 5, np.float32)\n",
    "    C_cpu_result_numba = np.zeros([A.shape[0], B.shape[1]], np.float32)\n",
    "    start_time = time.time()\n",
    "    result=matmul_cpu_numba(A, B, C_cpu_result_numba)\n",
    "    end_time = time.time()\n",
    "    del result\n",
    "    print( 'cpu time(Numba)', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过Numba调用CUDA开启GPU加速\n",
    "\n",
    "## 在编写CUDA核函数的时候，应该时刻有“并行”的思想\n",
    "\n",
    "## 每一个线程都会同时执行核函数，因此可以节省一些for循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu time(global memory) 0.9427287578582764\n"
     ]
    }
   ],
   "source": [
    "# 通过Numba调用CUDA开启GPU加速\n",
    "# 在编写CUDA核函数的时候，应该时刻有“并行”的思想\n",
    "# 每一个线程都会同时执行核函数，因此可以节省一些for循环\n",
    "@cuda.jit\n",
    "def matmul_gpu_global_mem(A, B, C):\n",
    "    # 利用CUDA Python的API获得当前线程在整个grid中的x、y索引\n",
    "    x, y = cuda.grid(2)\n",
    "    # 如果索引在有效范围内，没越界的话执行下面代码\n",
    "    # 实际经过测试发现，就算不写也不会报错，但从代码的完备性上来说还是写一下\n",
    "    if x < C.shape[0] and y < C.shape[1]:\n",
    "        tmp = 0.0\n",
    "        # 和前面类似，得到当前线程位置后，获取A、B中对应行列的元素相乘并累加\n",
    "        # A.shape[1]和B.shape[0]是等价的\n",
    "        for i in range(A.shape[1]):\n",
    "            tmp += A[x, i] * B[i, y]\n",
    "        C[x, y] = tmp\n",
    "if __name__ == '__main__':\n",
    "    A = np.full([2000, 3000], 3, np.float32)\n",
    "    B = np.full([3000, 2000], 5, np.float32)\n",
    "    # 将矩阵拷贝到GPU中方便处理\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    # 每个Block中的线程个数(一维)\n",
    "    TPB = 16\n",
    "    C_gpu_result_global = cuda.device_array((A.shape[0], B.shape[1]))\n",
    "    # 进行thread-block-grid配置\n",
    "    threadsperblock = (TPB, TPB)\n",
    "    # 注意这里两个shape都是int，所以得到的结果还是int，且有可能出现0\n",
    "    # 解决办法是将运算变成float类型，结果就是小数，再向上取整即可\n",
    "    blockspergrid_x = int(math.ceil(1.0 * A.shape[0] / threadsperblock[0]))\n",
    "    blockspergrid_y = int(math.ceil(1.0 * B.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 手动指定设置的thread-block-grid\n",
    "    result=matmul_gpu_global_mem[blockspergrid, threadsperblock](A_device, B_device, C_gpu_result_global)\n",
    "    # 可以同步等待一下，所有任务做完以后才会继续执行\n",
    "    cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    # 最后别忘了把device上的结果拷贝回来\n",
    "    C_cpu_result_global = C_gpu_result_global.copy_to_host(result)\n",
    "    print( 'gpu time(global memory)', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用shared memory加速读写操作，提高速度\n",
    "# 同一个block中的线程可以共享一小段shared memory，特点是读写非常快，但是内存较小\n",
    "# 我们可以将矩阵拆分成多个block，每个小块各自先进行乘法，最后再将所有结果累加，这样可以提升速度\n",
    "@cuda.jit\n",
    "def matmul_gpu_shared_mem(A, B, C):\n",
    "    TPB = 16\n",
    "    # 新建两个小的矩阵块对应block中的线程(所以大小和block一样)，数据类型为CDUA的float32\n",
    "    tA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    tB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    # 先获取到当前线程在整个grid中的坐标\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    # 再获取到当前线程在当前block中的坐标\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    # 简单进行越界判断\n",
    "    # 根据矩阵乘法，C的大小是A的行数(shape[0])、B的列数(shape[1])\n",
    "    if x > A.shape[0] or y > B.shape[1]:\n",
    "        return\n",
    "\n",
    "    # 计算一下按照当前tA、tB的大小需要计算多少次\n",
    "    exec_num = int(math.ceil(1.0 * A.shape[1] / TPB))\n",
    "    tmp = 0.0\n",
    "    # 每个线程都执行下面的循环\n",
    "    for i in range(exec_num):\n",
    "        # 首先根据当前线程位置，获取到A、B中的对应元素并赋给tA、tB对应位置\n",
    "        tA[tx, ty] = A[x, ty + i * TPB]\n",
    "        tB[tx, ty] = B[x + i * TPB, y]\n",
    "        # 因为核函数是所有线程并行执行的，所以这里我们需要等待一下\n",
    "        # 等待所有线程都执行完上面的操作后再继续进行\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # 到这里以后，tA、tB中所有的元素都被赋值了\n",
    "        # 所以我们就按照常规矩阵乘法，每个线程都执行行列对应相乘\n",
    "        for j in range(TPB):\n",
    "            tmp += tA[tx, j] * tB[j, ty]\n",
    "        # 和上面一样，要等待所有线程都执行完以后再继续\n",
    "        cuda.syncthreads()\n",
    "        # 执行完一次循环以后，一个拆分的小格子就被计算完了，再移动到下个block计算\n",
    "        # 上面所有计算的结果都累加到了tmp中\n",
    "\n",
    "    # 最后将tmp赋给C中的对应位置，结束\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 待计算的矩阵\n",
    "    # 需要注意的还是shape的顺序问题，shape[0]表示行方向，shape[1]表示列方向\n",
    "    A = np.full([700, 500], 3, np.float32)\n",
    "    B = np.full([500, 700], 5, np.float32)\n",
    "    # 将矩阵拷贝到GPU中方便处理\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    # 每个Block中的线程个数(一维)\n",
    "    TPB = 16\n",
    "\n",
    "    # 需要注意的是，CUDA加速只有在计算数据很大时才会有明显的效果\n",
    "    # 如果数据量很小，可能比CPU还慢(因为和CPU相比多了大量线程调度等操作)\n",
    "\n",
    "    # 方法1，最原始的算法\n",
    "    # --------------------------------------------------------------------------------\n",
    "    C_cpu_result = np.zeros([A.shape[0], B.shape[1]], np.float32)\n",
    "    start_time = time.time()\n",
    "    matmul_cpu(A, B, C_cpu_result)\n",
    "    end_time = time.time()\n",
    "    print ('cpu time', end_time - start_time)\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    # 方法2，利用Numba进行CPU加速\n",
    "    # --------------------------------------------------------------------------------\n",
    "    C_cpu_result_numba = np.zeros([A.shape[0], B.shape[1]], np.float32)\n",
    "    start_time = time.time()\n",
    "    matmul_cpu_numba(A, B, C_cpu_result_numba)\n",
    "    end_time = time.time()\n",
    "    print ('cpu time(Numba)', end_time - start_time)\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    # 方法3，利用CUDA进行GPU加速\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # 新建一个device上的矩阵用于存放结果\n",
    "    C_gpu_result_global = cuda.device_array((A.shape[0], B.shape[1]))\n",
    "\n",
    "    # 进行thread-block-grid配置\n",
    "    threadsperblock = (TPB, TPB)\n",
    "    # 注意这里两个shape都是int，所以得到的结果还是int，且有可能出现0\n",
    "    # 解决办法是将运算变成float类型，结果就是小数，再向上取整即可\n",
    "    blockspergrid_x = int(math.ceil(1.0 * A.shape[0] / threadsperblock[0]))\n",
    "    blockspergrid_y = int(math.ceil(1.0 * B.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 手动指定设置的thread-block-grid\n",
    "    matmul_gpu_global_mem[blockspergrid, threadsperblock](A_device, B_device, C_gpu_result_global)\n",
    "    # 可以同步等待一下，所有任务做完以后才会继续执行\n",
    "    cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    # 最后别忘了把device上的结果拷贝回来\n",
    "    C_cpu_result_global = C_gpu_result_global.copy_to_host()\n",
    "    print ('gpu time(global memory)', end_time - start_time)\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    # 方法4，利用CUDA和Shared Memory进一步提速\n",
    "    # 这里需要注意的是在数据量很小时，shared memory的优势可能体现不出来\n",
    "    # 因为根据设计的算法，是一个个小的block依次计算然后累加\n",
    "    # 和一个线程对应一个元素相比可能还会变慢\n",
    "    # --------------------------------------------------------------------------------\n",
    "    C_gpu_result_shared = cuda.device_array((A.shape[0], B.shape[1]))\n",
    "\n",
    "    threadsperblock = (TPB, TPB)\n",
    "    blockspergrid_x = int(math.ceil(1.0 * A.shape[0] / threadsperblock[0]))\n",
    "    blockspergrid_y = int(math.ceil(1.0 * B.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    start_time = time.time()\n",
    "    matmul_gpu_shared_mem[blockspergrid, threadsperblock](A_device, B_device, C_gpu_result_shared)\n",
    "    cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    C_cpu_result_shared = C_gpu_result_shared.copy_to_host()\n",
    "    print ('gpu time(shared memory)', end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
